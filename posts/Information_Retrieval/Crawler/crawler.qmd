---
title: "Crawling"
description: ""
date: "2023-09-29"
categories:
    - Information Retrieval
    - Crawler
---

## 1. 정의

- `crawling`: 자동으로 웹 페이지를 찾고 하는 작업
- `web crawler`: `crawling`을 수행하는 프로그램

## 2. Issue

### Preformance & Efficiency

---

#### distributed



### Scalability

---

### Search Strategy

---

1. Where to Start

2. Link ordering

- DFS (LIFO)
- BFS (FIFO)
- Best-first Algorithm
- Fish

3. Circularities

4. Duplicates

5. Checking for changes

### Robustness

---

- Be immune to `spider traps` and other malcious behavior from web servers

### Politeness

---

#### Explicit Politeness

##### `Robots.txt`
- 웹사이트의 어떤 부분을 크롤링해도 되는지에 대한 명세
- 웹사이트 관리자가 작성한다.
- 경로에 대한 접근 제한 명세
- 위치: 웹 서버 최상위 경로에 위치한다. e.g. <https://www.google.com/robots.txt>

**Example**
```text
User-agent: *
Disallow: /yoursite/temp/

User-agent: searchengine
Disallow: 
```

#### Implicit Politeness

- don't hit a server too often


- Respect implicit and explicit politeness considerations

### Parsing Pages for Links

---

### Malcious servers: SEOs

---

#### Spam Pages

#### Spider traps

- incl dynamically generated
- infinite URL names
- Ill-formed HTML
    - E.g. page with 68kB of null characters
- Misleading sites
    - indefinite number of pages dynamically generated by CGI scripts
    - paths of arbitrary depth created using soft directory links and path remapping features in HTTP server

**Solution**

- Check for URL length
- Guards
    - regular crawl statistics
    - Adding dominating sites to guard module
    - CGI form quaries와 같은 내용의 크롤링 비활성화
    - 텍스트 데이터 타입이 아닌 URL 제거

### Freshness

---

Continue fetching fresh copies of a previously fetched page

### Extensibility

---

Adapt to new data formats, protocols

### Storage

---

- 일반적인 HTML 웹 페이지는 압축 시 2-4 kB
    - [zlib](https://www.zlib.net/)
        - Deflate 압축 알고리즘을 C언어로 구현한 라이브러리
        - 비손실 압축 알고리즘
        - 높은 이식성
        - 압축 알고리즘계의 산업 표준

- 작은 규모의 시스템
    - `Storage Manager` 사용 (E.g. Berkeley DB)
    - 디스크 기반의 데이터베이스 관리
    - configuration as a hash-table/B-tree for URL access key
    - configuration as a sequential log of page records.

- 큰 규모의 시스템

## 4. Implementation

1. URL Frontier에 `seed` URL이 들어있다.
2. Frontier에서 URL을 하나 `추출`한다.
    2.1. URL에서 문서를 `Fetch`한다.
    2.1. `Parse` the URL
        - 문서에서 URL을 추출한다.
    2.2. 이미 방문한 URL인지 확인한다.
3. For each extracted URL
    3.1. URL 필터 테스트 (e.g. .edu 만 크롤링한다, robots.txt를 준수하는지)
    3.2. 이미 URL Frontier에 존재하는지 확인

## 5. Strategy

### URL Frontier

---

- 같은 호스트의 여러 페이지들을 포함할 수 있다.
- 모든 페이지들을 한 번에 Fetch 하는 것을 반드시 피해야 한다.
- 모든 크롤링 thread가 바빠야 한다.

### DNS (Domain Name Server)

---

- 인터넷의 lookup 서비스
    - URL이 주어지면, IP를 반환한다.
    - 서비스가 분산 서버에 의해 제공되므로 `lookup latency`가 높을 수 있다.
- DNS lookup 은 일반적으로 `blocking`으로 구현된다. - ??
    - only one outstanding request at a time
- 해결법
    - DNS caching
    - Batch DNS resolver
        - request들을 모아서 같이 전송

### fetching

---

- 한 번에 많은 페이지를 Fetch해야 함
- 가능한 많이 cache해야 함
    - DNS
    - robots.txt
    - Document (for later processing)
- Defensive
    - Time out http connections
    - `"crawler traps"` 
    - URL filter
    - Checkpointing

### Parsing

---

- Anchor tag: `<a href="URL"...>`
- Option tag: `<option value="URL"...>`
- Map: `<area href="URL"...>`
- Frame: `<frame src="URL"...>`
- Link to an image: `<img src="URL"...>`
- Absolute path: `<base href=...>`

#### URL normalization

- _relative_ URL 을 normalize(expand) 해준다.

### Filter

---

- `Regular Expression`
- Cache robots.txt files

### Distribution -- HOW?

---

- 서로 다른 작업들에 대해 다중 크롤러 스레드를 실행

### Duplicates

---

#### URL-seen test

#### Content-seen test

different URL, same document

- 웹 상에는 중복 문서가 다수 존재한다.
    - `Duplication`
        - **Exact match**
        - 많지 않음 
    - `Near-Duplication`
        - **Approximate match**
        - 다수 존재
        - E.g. Last modified date만 다른 경우
        - syntatic similarity 
        - edit-distance measure
        - similarity threshold
        - E.g. if Similarity > 80% => _near duplicates_


## Open Source Cralwers

### heritix

---

### apache nutch

---

### Scrapy

---

## Focused Crawler

### Issue

---

1. 관련 정보가 포함될 가능성이 가장 높은 URL 식별
2. 관련이 없거나 품질이 나쁜 문서를 피해야 함

### Techniques

---

#### 1. Lexical Based Approach

##### Fish Search System

Crawling algorithm based on depth-first search.
Heuristics determine the selection of the documents that are to be retrieved.

##### Shark Search System

Based on a fuzzy score to replace the binary relevant/irrelevant)
evaluation of document relevance, i.e., a score between 0 and 1 (0 for
no similarity whatsoever, 1 for perfect "conceptual" match) rather than
a binary value

##### Focused Crawler based on Category Taxonomy

Based on a category tree based document taxonomy and seed documents
which build a model for classification of retrieved pages into categories.
Uses semi-supervised learning

##### Focused Crawler based on Similarity Computing Engine

Retrieves information related to a user-specified topic from the Web using
TF.IDF weighting scheme to find the topic keywords set to represent the
topic, and uses vector space model to determine whether web pages are
relevant to the topic or not.

#### 2. Link Based Approach

##### Similarity based Crawler

Calculates the Page rank score on the graph induced by pages
downloaded so far and then using this score as a priority of URLs
extracted from a page.

##### HITS

Identifies the web page context based on Authority Score and Hub score
associated with the web page.
The basic principle here is the following mutually reinforcing
relationship between hubs and authorities. A good hub page points to
many good authority pages. A good authority page is pointed to by many
good hub pages.

##### ARC

Automatically compiles a resource list on any topic that is broad and
well-represented on the web.
Based on modified weighted Authority Score and Hub score.
Uses Anchor window wherenin the system looks on either side of the
href for a window of B bytes, to increase the authority weight.

##### DOM based Focused Crawler

Locates regions and subtrees of pages using DOM tree of a web page,
gets favorable treatment in propagating link-based popularity and
implicitly suppressing propagation of popularity to regions with noisy
links.
Identifies and extract hub regions relevant to a topic and guides the hub
and authority reinforcement to work on a selected, highly relevant
subgraph of the web.

##### Context Graph based Focused Crawler

Uses a context graph to train a set of classifiers to assign documents to
different categories based on their expected link distance to the target.
Naïve Bayes classifier is used for each layer of the graph.

## References

[Crawling and Duplicates] - Introduction to Information Retrieval CS276
[Content from the Web]
[FOCUSED CRAWLING TECHNIQUES](https://scienceon.kisti.re.kr/srch/selectPORSrchArticle.do?cn=NART110448811&SITE=CLICK)\
[Identifying and Filtering Near-Duplicate Documents](https://cs.brown.edu/courses/cs253/papers/nearduplicate.pdf)\
[Detecting Near-Duplicates for Web Crawling](https://static.googleusercontent.com/media/research.google.com/ko//pubs/archive/33026.pdf)\
[Identifying and Filtering Near-Duplicate Documents](https://cs.brown.edu/courses/cs253/papers/nearduplicate.pdf)\

[Web Crawling Algorithms](https://people.cse.nitc.ac.in/sites/default/files/aviralnigam/files/web_crawling_algorithms.pdf)\
[THE SHARK-SEARCH ALGORITHM](http://www.pelleg.org/hp/bin/360.html)\

[Optimized Focused Crawling for Web page Classification](https://ieeexplore-ieee-org.proxy.jbnu.ac.kr/stamp/stamp.jsp?tp=&arnumber=10209473)\
[Web Crawler and Web Crawler Algorithms: A Perspective]


[Heritrix 3.2.0 이용 매뉴얼](https://osasf.net/discussion/413/heritrix-3-2-0-%EC%9D%B4%EC%9A%A9-%EB%A7%A4%EB%89%B4%EC%96%BC)\
[Web Crawler의 정의](https://mech2cs.tistory.com/entry/Web-Crawler%EC%9D%98-%EC%A0%95%EC%9D%98-%EB%B0%8F-Apache-Nutch-%EA%B5%AC%EC%A1%B0)\
[오픈소스 웹크롤러 비교 - Naver blog](https://m.blog.naver.com/PostView.naver?isHttpsRedirect=true&blogId=2feelus&logNo=220476246170)\
[Focused crawling: a new approach to topic-specific Web resource discovery](http://dspace.library.iitb.ac.in/jspui/bitstream/10054/1305/1/6234-1.pdf)\
[Mercator](http://www.cs.cornell.edu/courses/cs685/2002fa/mercator.pdf)\

[Scrapy](https://docs.scrapy.org/en/latest/)\