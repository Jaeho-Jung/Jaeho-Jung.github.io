---
title: "Crawling Policy"
description: "크롤링 정책에 대해 설명한다."
date: "2023-10-3"
categories:
    - Information Retrieval
    - Crawler
---

## 크롤링 정책

1. `Selection policy`: 다운로드할 페이지를 지정하는 정책
2. `Re-visit policy`: 페이지 변경 확인 시기를 지정하는 정책
3. `Politeness policy`: 웹사이트 과부하를 피하는 방법을 지정하는 정책
4. `Parallelization policy`: 분산 웹 크롤러를 조율하는 방법하는 지정하는 정책

## Selection policy

웹의 현재 규모를 고려할 때 대형 검색 엔진조차도 공개적으로 이용 가능한 부분만을 다루고 있다. 2009년의 연구에 따르면 대형 검색 엔진도 색인 가능한 웹의 40–70%를 넘지 않았다. 크롤러는 항상 웹 페이지의 일부만을 다운로드하므로, 가장 관련성 높은 페이지를 다운로드하는 것이 중요하다. 페이지의 중요성은 본질적인 품질, 링크나 방문 횟수에 대한 인기, 심지어는 URL에 따라 달라진다.

### 링크 제한
   - 크롤러는 HTML 페이지만 찾고 다른 MIME 유형은 피하고 싶을 수 있다.
   - HTML 자원만 요청하려면 HTTP HEAD 요청을 사용하여 웹 자원의 MIME 유형을 확인한 후 GET 요청으로 전체 자원을 요청할 수 있다.
   - 많은 HEAD 요청을 피하기 위해 URL을 검사하여 URL이 특정 문자로 끝나는 경우에만 자원을 요청할 수 있다.
   - 이 전략은 의도하지 않게 많은 HTML 웹 자원을 건너뛸 수 있다.

### URL 정규화
   - 크롤러는 동일한 자원을 여러 번 크롤링을 피하기 위해 URL 정규화를 수행한다.
   - URL 정규화는 URL을 일관된 방식으로 수정하고 표준화하는 프로세스를 의미한다.
   - 소문자로 변환, "." 및 ".." 세그먼트 제거, 비어 있지 않은 경로 구성 요소에 대한 슬래시 추가 등 여러 유형의 정규화를 수행할 수 있다.

### 경로 상승 크롤링
   - 일부 크롤러는 특정 웹 사이트에서 가능한 많은 자원을 다운로드하려고 한다.
   - 경로 상승 크롤러는 크롤하려는 각 URL의 모든 경로로 상승하는 크롤러로 효과적으로 사용된다.
   - 예를 들어, 주어진 시드 URL이 http://llama.org/hamster/monkey/page.html이면 /hamster/monkey/, /hamster/, / 등을 크롤하려고 시도할 것이다.
   - 경로 상승 크롤러는 정규 크롤링에서 발견할 수 없는 고립된 자원이나 연결이 없는 자원을 효과적으로 찾는 데 사용된다.

### [포커스 크롤링](https://en.wikipedia.org/wiki/Focused_crawler)
   - 크롤러에 대한 페이지의 중요성은 페이지의 텍스트와 주어진 쿼리 간의 유사성 함수로 나타낼 수 있다.
   - 유사한 페이지를 다운로드하려는 웹 크롤러는 포커스 크롤러 또는 주제 크롤러라고 한다.
   - 포커스 크롤링은 페이지의 텍스트와 쿼리 간의 유사성을 실제로 페이지를 다운로드하기 전에 예측할 수 있는 능력이 주요한 문제이다.
   - 일부 예측자로는 링크의 앵커 텍스트가 있으며, 다른 방법으로는 이미 방문한 페이지의 완전한 콘텐츠를 사용하여 운전 쿼리와 아직 방문하지 않은 페이지 간의 유사성을 추론한다.

### 학술 포커스 크롤러
   - 학술 크롤러는 학문적인 관련 문서를 크롤링하는 예이다.
   - Citeseerxbot과 같은 학문 검색 엔진의 크롤러는 무료로 이용 가능한 학술 문서를 크롤링한다.
   - 학술 크롤러는 PDF, PostScript 파일, Microsoft Word 등과 같은 다양한 형식의 문서를 크롤링하는 데 관심이 있다.
   - 일반적인 오픈 소스 크롤러는 다른 MIME 유형을 필터링하기 위해 사용자 정의되어야 하거나 미들웨어를 사용하여 문서를 추출하고 포커스 크롤 데이터베이스로 가져와야 할 수 있다.

### 의미론적 포커스 크롤러
   - 의미론적 포커스 크롤러는 도메인 온톨로지를 사용하여 주제 지도를 나타내고 웹 페이지를 선택 및 분류하는 데 사용한다.
   - 온톨로지는 크롤링 프로세스에서 자동으로 업데이트될 수 있다.
   - 의미론적 온톨로지를 기반으로 하는 크롤러는 지식 벡터 머신을 사용하여 웹 페이지를 크롤하는 동안 온톨로지 개념의 내용을 업데이트하는 방법을 소개한다.

## Re-visit policy

웹은 매우 동적인 성격을 가지고 있다. 웹 크롤러가 크롤을 완료할 때 까지 create, update, delete 등의 이벤트가 발생할 수 있다.

검색 엔진 관점에서 이러한 이벤트를 감지하지 못하고, 오래된 리소스를 저장한다면 비용이 발생한다.

비용을 계산하는 함수로 흔히 `freshness`, `age`가 사용된다.

### Freshness

    - 로컬 복사본이 정확한지 여부를 나타내는 2진 측정치
    - 시간 t에 저장소에 있는 p의 freshness는 다음과 같이 정의된다.
$$
 F_{p}(t) = \begin{cases} 1 & \text{if } p \text{ is equal to the local copy at time } t \\ 0 & \text{otherwise} \end{cases} \
$$

### Age

    - 로컬 복사본이 얼마나 오래되었는지를 나타내는 측정치
    - 시간 t에 저장소에 있는 페이지 p의 age는 다음과 같이 정의된다.

$$
A_{p}(t) = \begin{cases} 0 & \text{if } p \text{ is not modified at time } t \\ t - \text{modification time of } p & \text{otherwise} \end{cases} \
$$

Coffman 등은 웹 크롤러가 서버이고 웹 사이트가 대기열인 다중 대기열, 단일 서버 폴링 시스템으로 모델링할 수 있다고 제안했다. 페이지 수정은 고객의 도착이고, 스위치 오버 시간은 단일 웹 사이트에 대한 페이지 액세스 간격이다. 이 모델에서 폴링 시스템의 평균 대기 시간은 웹 크롤러의 평균 age와 동일하다.

웹 크롤러의 목적은 컬렉션 내 페이지의 평균 freshness를 가능한 한 높이거나 페이지의 평균 age을 가능한 한 낮추는 것이다.

**웹 크롤러에서 신선도와 연령의 진화**\
Cho와 Garcia-Molina는 두 가지 간단한 re-visit policy를 연구했다:

1. **균일 정책:** 변화율과 상관없이 컬렉션의 모든 페이지를 동일한 빈도로 재방문한다.
2. **비례 정책:** 변화율이 높은 페이지를 더 자주 재방문한다. 방문 빈도는 추정된 변화 빈도에 비례한다.

양쪽 모두에서 페이지의 반복 크롤링 순서는 무작위 또는 고정 순서로 수행될 수 있다.

**결과**\
Cho와 Garcia-Molina는 균일 정책이 시뮬레이션 및 실제 웹 크롤에서 신선도의 평균 측면에서 비례 정책을 능가한다는 것을 입증했다.\
웹 크롤러는 주어진 시간 프레임 내에 크롤링 할 수 있는 페이지의 한계가 있기 때문에 \
1. 자주 업데이트되지 않는 페이지의 비용을 희생시켜 자주 업데이트되는 페이지에 지나치게 많은 새로운 크롤을 할당한다.\
2. 자주 변경되는 페이지의 신선도는 자주 변경되지 않는 페이지보다 짧다.

즉, 비례 정책은 자주 업데이트되는 페이지에 더 많은 리소스를 할당하지만 전반적인 freshness time은 적다.

신선도를 향상시키기 위해, 크롤러는 너무 자주 변경되는 요소에 벌을 주어야 한다. 

- 평균 freshness를 높이기 위한 최적의 방법은 너무 자주 변경되는 페이지를 무시하는 것이다.
- 평균 age를 낮추기 위한 최적의 방법은 각 페이지의 변경 속도와 단조적으로(하위 선형으로) 증가하는 액세스 빈도를 사용하는 것이다.
- 여기서 고려된 재방문 정책은 모든 페이지를 품질 측면에서 동질적으로 간주한다.- 웹 페이지 품질에 대한 추가 정보를 포함해야 더 나은 크롤링 정책을 달성할 수 있다.

## Politeness policy

웹 크롤러는 사람보다 훨씬 빠르고 깊은 수준의 데이터를 검색할 수 있기 때문에 사이트의 성능에 치명적인 영향을 미칠 수 있다. 단일 크롤러가 초당 여러 요청을 수행하거나 대용량 파일을 다운로드하는 경우, 서버는 여러 크롤러의 요청에 대응하기 어려울 수 있다.\

Koster에 따르면 웹 크롤러의 사용은 여러 작업에 유용하지만 일반 커뮤니티에는 비용이 따른다. 웹 크롤러 사용의 비용에는 다음이 포함됩니다:\

1. 네트워크 자원: 크롤러는 상당한 대역폭이 필요하며 긴 시간 동안 고도로 병렬로 작동합니다.

2. 서버 과부하: 특히 특정 서버로의 액세스 빈도가 너무 높은 경우.

3. 잘못 작성된 크롤러: 서버나 라우터를 충돌시키거나 처리할 수 없는 페이지를 다운로드하는 크롤러.

4. 개인 크롤러: 너무 많은 사용자가 배치되면 네트워크와 웹 서버를 방해할 수 있다.

:::{.callout-note appearance="simple"}

## robots.txt

이러한 문제에 대한 부분적인 해결책은 robots.txt 프로토콜로, 관리자가 크롤러에 의해 액세스되지 말아야 할 웹 서버의 부분을 지정하는 표준이다.\

최근 Google, Ask Jeeves, MSN 및 Yahoo! Search와 같은 상업용 검색 엔진은 robots.txt 파일에 `Crawl-delay:` 매개변수를 추가하여 요청 간의 지연 시간을 나타낼 수 있게 되었다.\

:::

**액세스 간격**
- Cho: 10초
- WIRE crawler: 15초
- MercatorWeb crawler: 적응적인 politeness policy
    - 특정 서버에서 문서를 다운로드하는 데 t초가 걸린 경우 다음 페이지를 다운로드하기 전에 10t초 기다린다.
- Dill:1초

연구 목적으로 웹 크롤러를 사용하는 경우 더 자세한 비용 대 혜택 분석이 필요하며 어디를 크롤할지, 얼마나 빨리 크롤할지 결정할 때 윤리적 측면을 고려해야 한다.

## Parallelization policy

- **병렬 크롤링:** 여러 프로세스를 사용하여 동시에 크롤링을 수행하는 방식.
- **목표:** 다운로드 속도를 극대화하고 병렬화로 인한 오버헤드를 최소화하며 동일한 페이지의 반복 다운로드를 방지.
- **필요성:** 동일한 URL이 두 개의 다른 크롤링 프로세스에서 발견될 수 있으므로, 새로 발견된 URL을 할당하는 정책이 필요함.


## References

<https://en.wikipedia.org/wiki/Web_crawler>\