[
  {
    "objectID": "posts/SSH-Key/Add_SSH_key_to_Github.html",
    "href": "posts/SSH-Key/Add_SSH_key_to_Github.html",
    "title": "Github에 SSH key 등록하기 for Windows",
    "section": "",
    "text": "시큐어 셸(Secure SHell, SSH)은 네트워크 상의 다른 컴퓨터에 로그인하거나 원격 시스템에서 명령을 실행하고 다른 시스템으로 파일을 복사할 수 있도록 해 주는 응용 프로그램 또는 그 프로토콜을 가리킨다.\n로컬 시스템에서 SSH (Secure Shell Protocol)를 사용하면 GitHub.com에 액세스할 수 있다. SSH를 통해 연결할 때, 로컬 시스템에 위치한 개인 키 파일을 이용해 인증한다."
  },
  {
    "objectID": "posts/SSH-Key/Add_SSH_key_to_Github.html#ssh란",
    "href": "posts/SSH-Key/Add_SSH_key_to_Github.html#ssh란",
    "title": "Github에 SSH key 등록하기 for Windows",
    "section": "",
    "text": "시큐어 셸(Secure SHell, SSH)은 네트워크 상의 다른 컴퓨터에 로그인하거나 원격 시스템에서 명령을 실행하고 다른 시스템으로 파일을 복사할 수 있도록 해 주는 응용 프로그램 또는 그 프로토콜을 가리킨다.\n로컬 시스템에서 SSH (Secure Shell Protocol)를 사용하면 GitHub.com에 액세스할 수 있다. SSH를 통해 연결할 때, 로컬 시스템에 위치한 개인 키 파일을 이용해 인증한다."
  },
  {
    "objectID": "posts/SSH-Key/Add_SSH_key_to_Github.html#ssh-key-생성",
    "href": "posts/SSH-Key/Add_SSH_key_to_Github.html#ssh-key-생성",
    "title": "Github에 SSH key 등록하기 for Windows",
    "section": "1. SSH key 생성",
    "text": "1. SSH key 생성\n\n1.1. Git for Windows 다운로드\nhttps://gitforwindows.org/에서 Git for Windows 툴을 다운로드한다.\n\n\n1.2. SSH Key 생성\n\n1.1. 에서 다운로드 한 Git Bash를 실행한다.\n다음 명령어를 입력한다. “your_email@example.com”에는 자신의 메일을 입력한다.\n\n\nssh-keygen -t ed25519 -C \"your_email@example.com\"\n\n\n\n\n\n\nNote\n\n\n\n레거시 시스템이 Ed25519 알고리즘을 지원하지 않는다면, 다음 명령어를 입력한다.\n\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\""
  },
  {
    "objectID": "posts/SSH-Key/Add_SSH_key_to_Github.html#ssh-agent에-ssh-key-등록",
    "href": "posts/SSH-Key/Add_SSH_key_to_Github.html#ssh-agent에-ssh-key-등록",
    "title": "Github에 SSH key 등록하기 for Windows",
    "section": "2. ssh-agent에 SSH Key 등록",
    "text": "2. ssh-agent에 SSH Key 등록\n\nssh-agent 백그라운드로 실행한다.\n\neval \"$(ssh-agent -s)\"\n\nssh-agent에 SSH Key를 등록한다.\n\nssh-add ~/.ssh/id_ed25519\n\n\n\n\n\n\nNote\n\n\n\nkey를 다른 이름으로 생성했거나, 이미 존재하는 key를 등록할 때에는 id_ed25519를 해당 key의 이름으로 바꾼다."
  },
  {
    "objectID": "posts/SSH-Key/Add_SSH_key_to_Github.html#github-계정에-ssh-key-등록",
    "href": "posts/SSH-Key/Add_SSH_key_to_Github.html#github-계정에-ssh-key-등록",
    "title": "Github에 SSH key 등록하기 for Windows",
    "section": "3. GitHub 계정에 SSH Key 등록",
    "text": "3. GitHub 계정에 SSH Key 등록\n\n클립보드에 SSH 공개 키를 복사한다.\n\nclip &lt; ~/.ssh/id_ed25519.pub\n\nGitHub에서 우측 상단의 프로필 사진을 클릭하고, Settings를 클릭한다.\nAccess 섹션의 SSH and GPG keys를 클릭한다.\nNew SSH key 혹은 Add SSH key를 클릭한다.\nTitle 필드에 새로운 키에 대한 설명 레이블을 입력한다. 예를 들어 개인용 노트북을 사용하는 경우 ’Personal laptop’라고 입력한다.\nKey type을 선택한다. Signing key에 대한 자세한 내용은 “About commit signature verification.”를 참고\nKey 필드에 복사한 공개 키를 붙여넣는다.\nAdd SSH key 버튼을 누른다."
  },
  {
    "objectID": "posts/SSH-Key/Add_SSH_key_to_Github.html#references",
    "href": "posts/SSH-Key/Add_SSH_key_to_Github.html#references",
    "title": "Github에 SSH key 등록하기 for Windows",
    "section": "References",
    "text": "References\nhttps://ko.wikipedia.org/wiki/%EC%8B%9C%ED%81%90%EC%96%B4_%EC%85%B8\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jaeho-Jung's blog",
    "section": "",
    "text": "Crawling\n\n\n\n\n\n\n\nInformation Retrieval\n\n\nCrawler\n\n\n\n\n\n\n\n\n\n\n\nSep 29, 2023\n\n\n\n\n\n\n  \n\n\n\n\nGithub에 SSH key 등록하기 for Windows\n\n\n\n\n\n\n\nguide\n\n\ngithub\n\n\nssh key\n\n\n\n\n\n\n\n\n\n\n\nSep 29, 2023\n\n\n\n\n\n\n  \n\n\n\n\nMarkdown 기초 문법\n\n\n\n\n\n\n\nguide\n\n\nmarkdown\n\n\nsyntax\n\n\n\n\nMarkdown 소개 및 기초 문법 정리\n\n\n\n\n\n\nSep 15, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Markdown/markdown_basics.html",
    "href": "posts/Markdown/markdown_basics.html",
    "title": "Markdown 기초 문법",
    "section": "",
    "text": "마크다운(Markdown)은 마크업 언어(Markup Language)의 일종으로, 2004년에 존 그루버(John Gruber)가 만들었다. 일반 텍스트로 서식이 있는 문서를 작성하는 데에 사용되며, 문법이 간단하여 읽고 쓰기 쉽다.\n\n\n\n\n\n\nNote\n\n\n\n파일 확장자는 .md, .markdown이다.\n\n\n\n\n\n\nVersatile: 마크다운은 웹사이트, 문서, 노트, 책, 프레젠테이션, 이메일 메시지 및 기술 문서 작성 등 다양한 용도로 사용할 수 있다.\nPortable: 마크다운 형식의 텍스트 파일은 거의 모든 응용 프로그램에서 열 수 있다.\nPlatform independent: 모든 운영 체제에서 마크다운 형식 텍스트를 생성할 수 있다.\nFuture-Proof: 애플리케이션이 작동을 멈춘다 하더라도 텍스트 편집 애플리케이션을 사용하여 마크다운 형식의 텍스트를 계속 읽을 수 있다.\nSupport: Reddit, GitHub 등 많은 데스크탑과 웹 기반 애플리케이션에서 마크다운을 지원한다."
  },
  {
    "objectID": "posts/Markdown/markdown_basics.html#개요",
    "href": "posts/Markdown/markdown_basics.html#개요",
    "title": "Markdown 기초 문법",
    "section": "",
    "text": "마크다운(Markdown)은 마크업 언어(Markup Language)의 일종으로, 2004년에 존 그루버(John Gruber)가 만들었다. 일반 텍스트로 서식이 있는 문서를 작성하는 데에 사용되며, 문법이 간단하여 읽고 쓰기 쉽다.\n\n\n\n\n\n\nNote\n\n\n\n파일 확장자는 .md, .markdown이다.\n\n\n\n\n\n\nVersatile: 마크다운은 웹사이트, 문서, 노트, 책, 프레젠테이션, 이메일 메시지 및 기술 문서 작성 등 다양한 용도로 사용할 수 있다.\nPortable: 마크다운 형식의 텍스트 파일은 거의 모든 응용 프로그램에서 열 수 있다.\nPlatform independent: 모든 운영 체제에서 마크다운 형식 텍스트를 생성할 수 있다.\nFuture-Proof: 애플리케이션이 작동을 멈춘다 하더라도 텍스트 편집 애플리케이션을 사용하여 마크다운 형식의 텍스트를 계속 읽을 수 있다.\nSupport: Reddit, GitHub 등 많은 데스크탑과 웹 기반 애플리케이션에서 마크다운을 지원한다."
  },
  {
    "objectID": "posts/Markdown/markdown_basics.html#문법",
    "href": "posts/Markdown/markdown_basics.html#문법",
    "title": "Markdown 기초 문법",
    "section": "문법",
    "text": "문법\n\n제목(Headings)\n\n\n\n\n\n\n\nMarkdown 문법\n출력\n\n\n\n\n# 제목 1\n제목 1\n\n\n## 제목 2\n제목 2\n\n\n### 제목 3\n제목 3\n\n\n#### 제목 4\n제목 4\n\n\n##### 제목 5\n제목 5\n\n\n###### 제목 6\n제목 6\n\n\n\n\n\n텍스트 포맷(Text Formatting)\n\n\n\n\n\n\n\nMarkdown 문법\n출력\n\n\n\n\n*이탤릭체*, **굵게**, ***굵은 이탤릭체***\nitalics, bold, bold italics\n\n\n위 첨자^2^ / 아래 첨자~2~\nsuperscript2 / subscript2\n\n\n~~취소선~~\nstrikethrough\n\n\n`코드 강조`\nverbatim code\n\n\n\n\n\n목록(Lists)\n\n1. 순서 있는 목록\n목록 숫자는 1로 시작해야 하며, 오름차순이 아니어도 된다.\n\n\n\n\n\n\n\nMarkdown 문법\n출력\n\n\n\n\n1. First item\n2. Second item\n3. Third item\n\nFirst item\nSecond item\nThird item\n\n\n\n1. First item\n1. Second item\n1. Third item\n\nFirst item\nSecond item\nThird item\n\n\n\n1. First item\n8. Second item\n5. Third item\n\nFirst item\nSecond item\nThird item\n\n\n\n1. First item\n2. Second item\n    1. Indented item\n    2. Indented item\n3. Third item\n\nFirst item\nSecond item\n\nIndent item\nIndent item\n\nThird item\n\n\n\n\n\n\n\n2. 순서 없는 목록\n-,*,+을 사용하여 순서 없는 목록을 생성할 수 있다.\n\n\n\n\n\n\n\nMarkdown 문법\n출력\n\n\n\n\n- First item\n- Second item\n- Third item\n\nFirst item\nSecond item\nThird item\n\n\n\n* First item\n* Second item\n* Third item\n\nFirst item\nSecond item\nThird item\n\n\n\n+ First item\n+ Second item\n+ Third item\n\nFirst item\nSecond item\nThird item\n\n\n\n- First item\n- Second item\n    - Indented item\n    - Indented item\n- Third item\n\nFirst item\nSecond item\n\nIndent item\nIndent item\n\nThird item\n\n\n\n\n\n\n\n3. 목록에 요소(Element) 추가\n목록에 다른 요소를 추가하려면 요소 앞에 들여쓰기 4칸을 삽입한다.\n* First item\n* Second item\n    - Indented item\n\n        &gt; Blockquote\n    \n    - Indented item\n* Third item\n\nFirst item\nSecond item\n\nIndented item\n\nBlockquote\n\nIndented item\n\nThird item\n\n\n\n\n\n블록(Blocks)\n\n1. 코드 블록(Code Blocks)\n\nFenced Code Blocks\n코드 위와 아래를 ``` 또는 ~~~로 감싸 코드 블럭을 만들 수 있다.\n```\na = 5\nif a % 2 == 0:\n  print(\"even\")\nelse:\n  print(\"odd\")\n```\na = 5\nif a % 2 == 0:\n  print(\"even\")\nelse:\n  print(\"odd\")\n\n\n구문 강조(Syntax Highlighting)\n구문 강조를 위해서 첫 ``` 뒤에 코드 작성에 사용된 언어를 적는다.\n```python\na = 5\nif a % 2 == 0:\n  print(\"even\")\nelse:\n  print(\"odd\")\n```\na = 5\nif a % 2 == 0:\n  print(\"even\")\nelse:\n  print(\"odd\")\n\n\n\n\n2. 인용문 블록(Blockquotes)\n&gt; Blockquotes\n\nBlockquotes\n\n&gt; Blockquotes\n&gt;\n&gt; with Multiple Paragraphs\n\nBlockquotes\nwith Multiple Paragraphs\n\n&gt; Blockquotes\n&gt;\n&gt;&gt; Nested Blockquotes\n\nBlockquotes\n\nNested Blockquotes\n\n\n&gt; #### Headings\n&gt;\n&gt; - Unordered Lists\n&gt; - Unordered Lists\n&gt;\n&gt; *italics* **bold**\n\nHeadings\n\nUnordered Lists\nUnordered Lists\n\nitalics bold\n\n\n\n\n\n\n링크 & 이미지(Links & Images)\n\n\n\nMarkdown 문법\n출력\n\n\n\n\n&lt;https://jaeho-jung.github.io/&gt;\nhttps://jaeho-jung.github.io/\n\n\n[Jaeho-Jung's Blog](https://jaeho-jung.github.io/)\nJaeho-Jung’s Blog\n\n\n![Caption](markdown.svg)\n\n\n\n[![Caption](markdown.svg)](https://jaeho-jung.github.io/)\n\n\n\n[![Caption](markdown.svg)](https://jaeho-jung.github.io/)\n\n\n\n[![](markdown.svg){fig-alt=\"Alt text\"}](https://jaeho-jung.github.io/)"
  },
  {
    "objectID": "posts/Markdown/markdown_basics.html#references",
    "href": "posts/Markdown/markdown_basics.html#references",
    "title": "Markdown 기초 문법",
    "section": "References",
    "text": "References\nhttps://www.markdownguide.org/getting-started/\nhttps://www.markdownguide.org/basic-syntax/\nhttps://www.markdownguide.org/extended-syntax/#fenced-code-blocks\nhttps://quarto.org/docs/authoring/markdown-basics.html\nhttps://ko.wikipedia.org/wiki/%EB%A7%88%ED%81%AC%EB%8B%A4%EC%9A%B4"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/crawler.html",
    "href": "posts/Information_Retrieval/Crawler/crawler.html",
    "title": "Crawling",
    "section": "",
    "text": "crawling: 자동으로 웹 페이지를 찾고 하는 작업\nweb crawler: crawling을 수행하는 프로그램"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/crawler.html#정의",
    "href": "posts/Information_Retrieval/Crawler/crawler.html#정의",
    "title": "Crawling",
    "section": "",
    "text": "crawling: 자동으로 웹 페이지를 찾고 하는 작업\nweb crawler: crawling을 수행하는 프로그램"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/crawler.html#issue",
    "href": "posts/Information_Retrieval/Crawler/crawler.html#issue",
    "title": "Crawling",
    "section": "2. Issue",
    "text": "2. Issue\n\nPreformance & Efficiency\n\n\ndistributed\n\n\n\nScalability\n\n\n\nSearch Strategy\n\n\nWhere to Start\nLink ordering\n\n\nDFS (LIFO)\nBFS (FIFO)\nBest-first Algorithm\nFish\n\n\nCircularities\nDuplicates\nChecking for changes\n\n\n\nRobustness\n\n\nBe immune to spider traps and other malcious behavior from web servers\n\n\n\nPoliteness\n\n\nExplicit Politeness\n\nRobots.txt\n\n웹사이트의 어떤 부분을 크롤링해도 되는지에 대한 명세\n웹사이트 관리자가 작성한다.\n경로에 대한 접근 제한 명세\n위치: 웹 서버 최상위 경로에 위치한다. e.g. https://www.google.com/robots.txt\n\nExample\nUser-agent: *\nDisallow: /yoursite/temp/\n\nUser-agent: searchengine\nDisallow: \n\n\n\nImplicit Politeness\n\ndon’t hit a server too often\nRespect implicit and explicit politeness considerations\n\n\n\n\nParsing Pages for Links\n\n\n\nMalcious servers: SEOs\n\n\nSpam Pages\n\n\nSpider traps\n\nincl dynamically generated\ninfinite URL names\nIll-formed HTML\n\nE.g. page with 68kB of null characters\n\nMisleading sites\n\nindefinite number of pages dynamically generated by CGI scripts\npaths of arbitrary depth created using soft directory links and path remapping features in HTTP server\n\n\nSolution\n\nCheck for URL length\nGuards\n\nregular crawl statistics\nAdding dominating sites to guard module\nCGI form quaries와 같은 내용의 크롤링 비활성화\n텍스트 데이터 타입이 아닌 URL 제거\n\n\n\n\n\nFreshness\n\nContinue fetching fresh copies of a previously fetched page\n\n\nExtensibility\n\nAdapt to new data formats, protocols\n\n\nStorage\n\n\n일반적인 HTML 웹 페이지는 압축 시 2-4 kB\n\nzlib\n\nDeflate 압축 알고리즘을 C언어로 구현한 라이브러리\n비손실 압축 알고리즘\n높은 이식성\n압축 알고리즘계의 산업 표준\n\n\n작은 규모의 시스템\n\nStorage Manager 사용 (E.g. Berkeley DB)\n디스크 기반의 데이터베이스 관리\nconfiguration as a hash-table/B-tree for URL access key\nconfiguration as a sequential log of page records.\n\n큰 규모의 시스템"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/crawler.html#strategy",
    "href": "posts/Information_Retrieval/Crawler/crawler.html#strategy",
    "title": "Crawling",
    "section": "5. Strategy",
    "text": "5. Strategy\n\nURL Frontier\n\n\n같은 호스트의 여러 페이지들을 포함할 수 있다.\n모든 페이지들을 한 번에 Fetch 하는 것을 반드시 피해야 한다.\n모든 크롤링 thread가 바빠야 한다.\n\n\n\nDNS (Domain Name Server)\n\n\n인터넷의 lookup 서비스\n\nURL이 주어지면, IP를 반환한다.\n서비스가 분산 서버에 의해 제공되므로 lookup latency가 높을 수 있다.\n\nDNS lookup 은 일반적으로 blocking으로 구현된다. - ??\n\nonly one outstanding request at a time\n\n해결법\n\nDNS caching\nBatch DNS resolver\n\nrequest들을 모아서 같이 전송\n\n\n\n\n\nfetching\n\n\n한 번에 많은 페이지를 Fetch해야 함\n가능한 많이 cache해야 함\n\nDNS\nrobots.txt\nDocument (for later processing)\n\nDefensive\n\nTime out http connections\n\"crawler traps\"\nURL filter\nCheckpointing\n\n\n\n\nParsing\n\n\nAnchor tag: &lt;a href=\"URL\"...&gt;\nOption tag: &lt;option value=\"URL\"...&gt;\nMap: &lt;area href=\"URL\"...&gt;\nFrame: &lt;frame src=\"URL\"...&gt;\nLink to an image: &lt;img src=\"URL\"...&gt;\nAbsolute path: &lt;base href=...&gt;\n\n\nURL normalization\n\nrelative URL 을 normalize(expand) 해준다.\n\n\n\n\nFilter\n\n\nRegular Expression\nCache robots.txt files\n\n\n\nDistribution – HOW?\n\n\n서로 다른 작업들에 대해 다중 크롤러 스레드를 실행\n\n\n\nDuplicates\n\n\nURL-seen test\n\n\nContent-seen test\ndifferent URL, same document\n\n웹 상에는 중복 문서가 다수 존재한다.\n\nDuplication\n\nExact match\n많지 않음\n\nNear-Duplication\n\nApproximate match\n다수 존재\nE.g. Last modified date만 다른 경우\nsyntatic similarity\nedit-distance measure\nsimilarity threshold\nE.g. if Similarity &gt; 80% =&gt; near duplicates"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/crawler.html#implementation",
    "href": "posts/Information_Retrieval/Crawler/crawler.html#implementation",
    "title": "Crawling",
    "section": "4. Implementation",
    "text": "4. Implementation\n\nURL Frontier에 seed URL이 들어있다.\nFrontier에서 URL을 하나 추출한다. 2.1. URL에서 문서를 Fetch한다. 2.1. Parse the URL - 문서에서 URL을 추출한다. 2.2. 이미 방문한 URL인지 확인한다.\nFor each extracted URL 3.1. URL 필터 테스트 (e.g. .edu 만 크롤링한다, robots.txt를 준수하는지) 3.2. 이미 URL Frontier에 존재하는지 확인"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/crawler.html#open-source-cralwers",
    "href": "posts/Information_Retrieval/Crawler/crawler.html#open-source-cralwers",
    "title": "Crawling",
    "section": "Open Source Cralwers",
    "text": "Open Source Cralwers\n\nheritix\n\n\n\napache nutch\n\n\n\nScrapy"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/crawler.html#focused-crawler",
    "href": "posts/Information_Retrieval/Crawler/crawler.html#focused-crawler",
    "title": "Crawling",
    "section": "Focused Crawler",
    "text": "Focused Crawler\n\nIssue\n\n\n관련 정보가 포함될 가능성이 가장 높은 URL 식별\n관련이 없거나 품질이 나쁜 문서를 피해야 함\n\n\n\nTechniques\n\n\n1. Lexical Based Approach\n\nFish Search System\nCrawling algorithm based on depth-first search. Heuristics determine the selection of the documents that are to be retrieved.\n\n\nShark Search System\nBased on a fuzzy score to replace the binary relevant/irrelevant) evaluation of document relevance, i.e., a score between 0 and 1 (0 for no similarity whatsoever, 1 for perfect “conceptual” match) rather than a binary value\n\n\nFocused Crawler based on Category Taxonomy\nBased on a category tree based document taxonomy and seed documents which build a model for classification of retrieved pages into categories. Uses semi-supervised learning\n\n\nFocused Crawler based on Similarity Computing Engine\nRetrieves information related to a user-specified topic from the Web using TF.IDF weighting scheme to find the topic keywords set to represent the topic, and uses vector space model to determine whether web pages are relevant to the topic or not.\n\n\n\n2. Link Based Approach\n\nSimilarity based Crawler\nCalculates the Page rank score on the graph induced by pages downloaded so far and then using this score as a priority of URLs extracted from a page.\n\n\nHITS\nIdentifies the web page context based on Authority Score and Hub score associated with the web page. The basic principle here is the following mutually reinforcing relationship between hubs and authorities. A good hub page points to many good authority pages. A good authority page is pointed to by many good hub pages.\n\n\nARC\nAutomatically compiles a resource list on any topic that is broad and well-represented on the web. Based on modified weighted Authority Score and Hub score. Uses Anchor window wherenin the system looks on either side of the href for a window of B bytes, to increase the authority weight.\n\n\nDOM based Focused Crawler\nLocates regions and subtrees of pages using DOM tree of a web page, gets favorable treatment in propagating link-based popularity and implicitly suppressing propagation of popularity to regions with noisy links. Identifies and extract hub regions relevant to a topic and guides the hub and authority reinforcement to work on a selected, highly relevant subgraph of the web.\n\n\nContext Graph based Focused Crawler\nUses a context graph to train a set of classifiers to assign documents to different categories based on their expected link distance to the target. Naïve Bayes classifier is used for each layer of the graph."
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/crawler.html#references",
    "href": "posts/Information_Retrieval/Crawler/crawler.html#references",
    "title": "Crawling",
    "section": "References",
    "text": "References\n[Crawling and Duplicates] - Introduction to Information Retrieval CS276 [Content from the Web] FOCUSED CRAWLING TECHNIQUES\nIdentifying and Filtering Near-Duplicate Documents\nDetecting Near-Duplicates for Web Crawling\nIdentifying and Filtering Near-Duplicate Documents\n\nWeb Crawling Algorithms\nTHE SHARK-SEARCH ALGORITHM\n\nOptimized Focused Crawling for Web page Classification\n[Web Crawler and Web Crawler Algorithms: A Perspective]\nHeritrix 3.2.0 이용 매뉴얼\nWeb Crawler의 정의\n오픈소스 웹크롤러 비교 - Naver blog\nFocused crawling: a new approach to topic-specific Web resource discovery\nMercator\n\nScrapy"
  }
]