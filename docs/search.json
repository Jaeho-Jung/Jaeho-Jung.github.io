[
  {
    "objectID": "posts/SSH-Key/Add_SSH_key_to_Github.html",
    "href": "posts/SSH-Key/Add_SSH_key_to_Github.html",
    "title": "Github에 SSH key 등록하기 for Windows",
    "section": "",
    "text": "시큐어 셸(Secure SHell, SSH)은 네트워크 상의 다른 컴퓨터에 로그인하거나 원격 시스템에서 명령을 실행하고 다른 시스템으로 파일을 복사할 수 있도록 해 주는 응용 프로그램 또는 그 프로토콜을 가리킨다.\n로컬 시스템에서 SSH (Secure Shell Protocol)를 사용하면 GitHub.com에 액세스할 수 있다. SSH를 통해 연결할 때, 로컬 시스템에 위치한 개인 키 파일을 이용해 인증한다."
  },
  {
    "objectID": "posts/SSH-Key/Add_SSH_key_to_Github.html#ssh란",
    "href": "posts/SSH-Key/Add_SSH_key_to_Github.html#ssh란",
    "title": "Github에 SSH key 등록하기 for Windows",
    "section": "",
    "text": "시큐어 셸(Secure SHell, SSH)은 네트워크 상의 다른 컴퓨터에 로그인하거나 원격 시스템에서 명령을 실행하고 다른 시스템으로 파일을 복사할 수 있도록 해 주는 응용 프로그램 또는 그 프로토콜을 가리킨다.\n로컬 시스템에서 SSH (Secure Shell Protocol)를 사용하면 GitHub.com에 액세스할 수 있다. SSH를 통해 연결할 때, 로컬 시스템에 위치한 개인 키 파일을 이용해 인증한다."
  },
  {
    "objectID": "posts/SSH-Key/Add_SSH_key_to_Github.html#ssh-key-생성",
    "href": "posts/SSH-Key/Add_SSH_key_to_Github.html#ssh-key-생성",
    "title": "Github에 SSH key 등록하기 for Windows",
    "section": "1. SSH key 생성",
    "text": "1. SSH key 생성\n\n1.1. Git for Windows 다운로드\nhttps://gitforwindows.org/에서 Git for Windows 툴을 다운로드한다.\n\n\n1.2. SSH Key 생성\n\n1.1. 에서 다운로드 한 Git Bash를 실행한다.\n다음 명령어를 입력한다. “your_email@example.com”에는 자신의 메일을 입력한다.\n\n\nssh-keygen -t ed25519 -C \"your_email@example.com\"\n\n\n\n\n\n\nNote\n\n\n\n레거시 시스템이 Ed25519 알고리즘을 지원하지 않는다면, 다음 명령어를 입력한다.\n\nssh-keygen -t rsa -b 4096 -C \"your_email@example.com\""
  },
  {
    "objectID": "posts/SSH-Key/Add_SSH_key_to_Github.html#ssh-agent에-ssh-key-등록",
    "href": "posts/SSH-Key/Add_SSH_key_to_Github.html#ssh-agent에-ssh-key-등록",
    "title": "Github에 SSH key 등록하기 for Windows",
    "section": "2. ssh-agent에 SSH Key 등록",
    "text": "2. ssh-agent에 SSH Key 등록\n\nssh-agent 백그라운드로 실행한다.\n\neval \"$(ssh-agent -s)\"\n\nssh-agent에 SSH Key를 등록한다.\n\nssh-add ~/.ssh/id_ed25519\n\n\n\n\n\n\nNote\n\n\n\nkey를 다른 이름으로 생성했거나, 이미 존재하는 key를 등록할 때에는 id_ed25519를 해당 key의 이름으로 바꾼다."
  },
  {
    "objectID": "posts/SSH-Key/Add_SSH_key_to_Github.html#github-계정에-ssh-key-등록",
    "href": "posts/SSH-Key/Add_SSH_key_to_Github.html#github-계정에-ssh-key-등록",
    "title": "Github에 SSH key 등록하기 for Windows",
    "section": "3. GitHub 계정에 SSH Key 등록",
    "text": "3. GitHub 계정에 SSH Key 등록\n\n클립보드에 SSH 공개 키를 복사한다.\n\nclip &lt; ~/.ssh/id_ed25519.pub\n\nGitHub에서 우측 상단의 프로필 사진을 클릭하고, Settings를 클릭한다.\nAccess 섹션의 SSH and GPG keys를 클릭한다.\nNew SSH key 혹은 Add SSH key를 클릭한다.\nTitle 필드에 새로운 키에 대한 설명 레이블을 입력한다. 예를 들어 개인용 노트북을 사용하는 경우 ’Personal laptop’라고 입력한다.\nKey type을 선택한다. Signing key에 대한 자세한 내용은 “About commit signature verification.”를 참고\nKey 필드에 복사한 공개 키를 붙여넣는다.\nAdd SSH key 버튼을 누른다."
  },
  {
    "objectID": "posts/SSH-Key/Add_SSH_key_to_Github.html#references",
    "href": "posts/SSH-Key/Add_SSH_key_to_Github.html#references",
    "title": "Github에 SSH key 등록하기 for Windows",
    "section": "References",
    "text": "References\nhttps://ko.wikipedia.org/wiki/%EC%8B%9C%ED%81%90%EC%96%B4_%EC%85%B8\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/Scrapy.html",
    "href": "posts/Information_Retrieval/Crawler/Scrapy.html",
    "title": "Scrapy",
    "section": "",
    "text": "Scrapy 2.11"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/Scrapy.html#버전",
    "href": "posts/Information_Retrieval/Crawler/Scrapy.html#버전",
    "title": "Scrapy",
    "section": "",
    "text": "Scrapy 2.11"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/Scrapy.html#settings",
    "href": "posts/Information_Retrieval/Crawler/Scrapy.html#settings",
    "title": "Scrapy",
    "section": "Settings",
    "text": "Settings\n\n설정 방법\n\n\nBuilt-in settings References\n\nBOT_NAME\nDefault: 'scrapybot'\nScrapy 프로젝트의 기본 봇 이름이다.\n\n\nCONCURRENT_ITEMS\nDefault: 100\nitem pipeline에 병렬적으로 동시에 처리할 수 있는 요소의 최댓값이다.\n\n\nCONCURRENT_REQUESTS\nDefault: 16\nScrapy downloader에 의해 수행 될 동시 요청 개수의 최댓값이다.\nThe maximum number of concurrent (i.e. simultaneous) requests that will be performed by the Scrapy downloader.\n\n\nCONCURRENT_REQUESTS_PER_DOMAIN\nDefault: 8\n하나의 도메인에 의해 수행 될 동시 요청 개수의 최댓값이다.\n\n\nCONCURRENT_REQUESTS_PER_IP\nDefault: 0\n하나의 IP에 의해 수행 될 동시 요청 개수의 최댓값이다. 0이 아니면, CONCURRENT_REQUESTS_PER_DOMAIN 설정값은 무시되고 이 값만 사용된다. =&gt; 동시성의 한계값은 도메인이 아닌 IP에 의해 적용된다.\n\n\nDEFAULT_REQUEST_HEADERS\nDefault:\n{\n    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n    \"Accept-Language\": \"en\",\n}\n\n\nDEPTH_LIMIT\nDefault: 0\nScope: scrapy.spidermiddlewares.depth.DepthMiddleware\n크롤링 최대 허용 깊이,"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/Scrapy.html#references",
    "href": "posts/Information_Retrieval/Crawler/Scrapy.html#references",
    "title": "Scrapy",
    "section": "References",
    "text": "References\nhttps://docs.scrapy.org/en/latest/index.html"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jaeho-Jung's blog",
    "section": "",
    "text": "Crawler의 정의\n\n\n\n\n\n\n\nInformation Retrieval\n\n\nCrawler\n\n\n\n\nCrawler의 정의와 평가 방법에 대해 설명한다.\n\n\n\n\n\n\nOct 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nCrawler의 Issue\n\n\n\n\n\n\n\nInformation Retrieval\n\n\nCrawler\n\n\n\n\nCrawling의 Issue에 대해 설명한다.\n\n\n\n\n\n\nOct 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nCrawling Policy\n\n\n\n\n\n\n\nInformation Retrieval\n\n\nCrawler\n\n\n\n\n크롤링 정책에 대해 설명한다.\n\n\n\n\n\n\nOct 3, 2023\n\n\n\n\n\n\n  \n\n\n\n\nScrapy\n\n\n\n\n\n\n\nInformation Retrieval\n\n\nCrawler\n\n\nScrapy\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2023\n\n\n\n\n\n\n  \n\n\n\n\nGithub에 SSH key 등록하기 for Windows\n\n\n\n\n\n\n\nguide\n\n\ngithub\n\n\nssh key\n\n\n\n\n\n\n\n\n\n\n\nSep 29, 2023\n\n\n\n\n\n\n  \n\n\n\n\nMarkdown 기초 문법\n\n\n\n\n\n\n\nguide\n\n\nmarkdown\n\n\nsyntax\n\n\n\n\nMarkdown 소개 및 기초 문법 정리\n\n\n\n\n\n\nSep 15, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/Crawler_definition.html",
    "href": "posts/Information_Retrieval/Crawler/Crawler_definition.html",
    "title": "Crawler의 정의",
    "section": "",
    "text": "자동으로 웹페이지를 찾고 다운로드하는 프로그램\n\ncrawling: 자동으로 웹 페이지를 찾고 하는 작업\nweb crawler: crawling을 수행하는 프로그램\nSpider: 거미가 거미줄을 기어다니듯이 검색 엔진 봇이 “Web” 전체를 기어다닌다.\n\n\n\n\n\n웹 크롤러 봇은, 정리되지 않은 도서관의 모든 책을 검토하고 카드 카탈로그를 구성함으로써, 도서관을 찾는 이가 필요한 정보를 빠르고 쉽게 찾을 수 있도록 도와주는 사람과 유사합니다. 이 사람은 도서관의 책을 주제별로 분류하고 정렬할 수 있도록, 책의 제목, 요약, 본문 중 일부를 읽어 무엇에 대한 책인지 파악할 것입니다.\n\n\n\n웹 크롤러란? | 웹 스파이더의 작동 원리, Cloudflare"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/Crawler_definition.html#정의",
    "href": "posts/Information_Retrieval/Crawler/Crawler_definition.html#정의",
    "title": "Crawler의 정의",
    "section": "",
    "text": "자동으로 웹페이지를 찾고 다운로드하는 프로그램\n\ncrawling: 자동으로 웹 페이지를 찾고 하는 작업\nweb crawler: crawling을 수행하는 프로그램\nSpider: 거미가 거미줄을 기어다니듯이 검색 엔진 봇이 “Web” 전체를 기어다닌다.\n\n\n\n\n\n웹 크롤러 봇은, 정리되지 않은 도서관의 모든 책을 검토하고 카드 카탈로그를 구성함으로써, 도서관을 찾는 이가 필요한 정보를 빠르고 쉽게 찾을 수 있도록 도와주는 사람과 유사합니다. 이 사람은 도서관의 책을 주제별로 분류하고 정렬할 수 있도록, 책의 제목, 요약, 본문 중 일부를 읽어 무엇에 대한 책인지 파악할 것입니다.\n\n\n\n웹 크롤러란? | 웹 스파이더의 작동 원리, Cloudflare"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/Crawler_definition.html#평가-방법",
    "href": "posts/Information_Retrieval/Crawler/Crawler_definition.html#평가-방법",
    "title": "Crawler의 정의",
    "section": "평가 방법",
    "text": "평가 방법\n\nFreshness\n\n웹 상의 문서들은 create, update, delete의 동작을 반복한다. 크롤러는 이러한 문서들의 변경 사항들을 빠르게 수집해야 한다.\n웹의 규모는 매우 크며 매 순간 생성되고 있기 때문에 얼마나 많은 웹페이지가 존재하는지 측정할 수 없다.\n대부분의 조직들은 웹페이지를 저장할 수 있는 저장공간이 부족하지만 freshness를 유지하기 위해 끊임없이 새 문서들을 저장해야 한다.\n\n\n\nCoverage\n\n검색엔진의 관점에서 크롤러가 얼마나 많은 웹 페이지를 수집하는지는 중요한 척도가 된다.\n웹의 규모는 매우 크기 때문에, 유효한 문서를 효율적으로 찾아 저장해야 한다."
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/Crawler_definition.html#references",
    "href": "posts/Information_Retrieval/Crawler/Crawler_definition.html#references",
    "title": "Crawler의 정의",
    "section": "References",
    "text": "References\n\nSEIRiP\n\n웹 크롤러란? | 웹 스파이더의 작동 원리 - Cloudflare\n\nWeb Crawler - Wikipedia"
  },
  {
    "objectID": "posts/Markdown/markdown_basics.html",
    "href": "posts/Markdown/markdown_basics.html",
    "title": "Markdown 기초 문법",
    "section": "",
    "text": "마크다운(Markdown)은 마크업 언어(Markup Language)의 일종으로, 2004년에 존 그루버(John Gruber)가 만들었다. 일반 텍스트로 서식이 있는 문서를 작성하는 데에 사용되며, 문법이 간단하여 읽고 쓰기 쉽다.\n\n\n\n\n\n\nNote\n\n\n\n파일 확장자는 .md, .markdown이다.\n\n\n\n\n\n\nVersatile: 마크다운은 웹사이트, 문서, 노트, 책, 프레젠테이션, 이메일 메시지 및 기술 문서 작성 등 다양한 용도로 사용할 수 있다.\nPortable: 마크다운 형식의 텍스트 파일은 거의 모든 응용 프로그램에서 열 수 있다.\nPlatform independent: 모든 운영 체제에서 마크다운 형식 텍스트를 생성할 수 있다.\nFuture-Proof: 애플리케이션이 작동을 멈춘다 하더라도 텍스트 편집 애플리케이션을 사용하여 마크다운 형식의 텍스트를 계속 읽을 수 있다.\nSupport: Reddit, GitHub 등 많은 데스크탑과 웹 기반 애플리케이션에서 마크다운을 지원한다."
  },
  {
    "objectID": "posts/Markdown/markdown_basics.html#개요",
    "href": "posts/Markdown/markdown_basics.html#개요",
    "title": "Markdown 기초 문법",
    "section": "",
    "text": "마크다운(Markdown)은 마크업 언어(Markup Language)의 일종으로, 2004년에 존 그루버(John Gruber)가 만들었다. 일반 텍스트로 서식이 있는 문서를 작성하는 데에 사용되며, 문법이 간단하여 읽고 쓰기 쉽다.\n\n\n\n\n\n\nNote\n\n\n\n파일 확장자는 .md, .markdown이다.\n\n\n\n\n\n\nVersatile: 마크다운은 웹사이트, 문서, 노트, 책, 프레젠테이션, 이메일 메시지 및 기술 문서 작성 등 다양한 용도로 사용할 수 있다.\nPortable: 마크다운 형식의 텍스트 파일은 거의 모든 응용 프로그램에서 열 수 있다.\nPlatform independent: 모든 운영 체제에서 마크다운 형식 텍스트를 생성할 수 있다.\nFuture-Proof: 애플리케이션이 작동을 멈춘다 하더라도 텍스트 편집 애플리케이션을 사용하여 마크다운 형식의 텍스트를 계속 읽을 수 있다.\nSupport: Reddit, GitHub 등 많은 데스크탑과 웹 기반 애플리케이션에서 마크다운을 지원한다."
  },
  {
    "objectID": "posts/Markdown/markdown_basics.html#문법",
    "href": "posts/Markdown/markdown_basics.html#문법",
    "title": "Markdown 기초 문법",
    "section": "문법",
    "text": "문법\n\n제목(Headings)\n\n\n\n\n\n\n\nMarkdown 문법\n출력\n\n\n\n\n# 제목 1\n제목 1\n\n\n## 제목 2\n제목 2\n\n\n### 제목 3\n제목 3\n\n\n#### 제목 4\n제목 4\n\n\n##### 제목 5\n제목 5\n\n\n###### 제목 6\n제목 6\n\n\n\n\n\n텍스트 포맷(Text Formatting)\n\n\n\n\n\n\n\nMarkdown 문법\n출력\n\n\n\n\n*이탤릭체*, **굵게**, ***굵은 이탤릭체***\nitalics, bold, bold italics\n\n\n위 첨자^2^ / 아래 첨자~2~\nsuperscript2 / subscript2\n\n\n~~취소선~~\nstrikethrough\n\n\n`코드 강조`\nverbatim code\n\n\n\n\n\n목록(Lists)\n\n1. 순서 있는 목록\n목록 숫자는 1로 시작해야 하며, 오름차순이 아니어도 된다.\n\n\n\n\n\n\n\nMarkdown 문법\n출력\n\n\n\n\n1. First item\n2. Second item\n3. Third item\n\nFirst item\nSecond item\nThird item\n\n\n\n1. First item\n1. Second item\n1. Third item\n\nFirst item\nSecond item\nThird item\n\n\n\n1. First item\n8. Second item\n5. Third item\n\nFirst item\nSecond item\nThird item\n\n\n\n1. First item\n2. Second item\n    1. Indented item\n    2. Indented item\n3. Third item\n\nFirst item\nSecond item\n\nIndent item\nIndent item\n\nThird item\n\n\n\n\n\n\n\n2. 순서 없는 목록\n-,*,+을 사용하여 순서 없는 목록을 생성할 수 있다.\n\n\n\n\n\n\n\nMarkdown 문법\n출력\n\n\n\n\n- First item\n- Second item\n- Third item\n\nFirst item\nSecond item\nThird item\n\n\n\n* First item\n* Second item\n* Third item\n\nFirst item\nSecond item\nThird item\n\n\n\n+ First item\n+ Second item\n+ Third item\n\nFirst item\nSecond item\nThird item\n\n\n\n- First item\n- Second item\n    - Indented item\n    - Indented item\n- Third item\n\nFirst item\nSecond item\n\nIndent item\nIndent item\n\nThird item\n\n\n\n\n\n\n\n3. 목록에 요소(Element) 추가\n목록에 다른 요소를 추가하려면 요소 앞에 들여쓰기 4칸을 삽입한다.\n* First item\n* Second item\n    - Indented item\n\n        &gt; Blockquote\n    \n    - Indented item\n* Third item\n\nFirst item\nSecond item\n\nIndented item\n\nBlockquote\n\nIndented item\n\nThird item\n\n\n\n\n\n블록(Blocks)\n\n1. 코드 블록(Code Blocks)\n\nFenced Code Blocks\n코드 위와 아래를 ``` 또는 ~~~로 감싸 코드 블럭을 만들 수 있다.\n```\na = 5\nif a % 2 == 0:\n  print(\"even\")\nelse:\n  print(\"odd\")\n```\na = 5\nif a % 2 == 0:\n  print(\"even\")\nelse:\n  print(\"odd\")\n\n\n구문 강조(Syntax Highlighting)\n구문 강조를 위해서 첫 ``` 뒤에 코드 작성에 사용된 언어를 적는다.\n```python\na = 5\nif a % 2 == 0:\n  print(\"even\")\nelse:\n  print(\"odd\")\n```\na = 5\nif a % 2 == 0:\n  print(\"even\")\nelse:\n  print(\"odd\")\n\n\n\n\n2. 인용문 블록(Blockquotes)\n&gt; Blockquotes\n\nBlockquotes\n\n&gt; Blockquotes\n&gt;\n&gt; with Multiple Paragraphs\n\nBlockquotes\nwith Multiple Paragraphs\n\n&gt; Blockquotes\n&gt;\n&gt;&gt; Nested Blockquotes\n\nBlockquotes\n\nNested Blockquotes\n\n\n&gt; #### Headings\n&gt;\n&gt; - Unordered Lists\n&gt; - Unordered Lists\n&gt;\n&gt; *italics* **bold**\n\nHeadings\n\nUnordered Lists\nUnordered Lists\n\nitalics bold\n\n\n\n\n\n\n링크 & 이미지(Links & Images)\n\n\n\nMarkdown 문법\n출력\n\n\n\n\n&lt;https://jaeho-jung.github.io/&gt;\nhttps://jaeho-jung.github.io/\n\n\n[Jaeho-Jung's Blog](https://jaeho-jung.github.io/)\nJaeho-Jung’s Blog\n\n\n![Caption](markdown.svg)\n\n\n\n[![Caption](markdown.svg)](https://jaeho-jung.github.io/)\n\n\n\n[![Caption](markdown.svg)](https://jaeho-jung.github.io/)\n\n\n\n[![](markdown.svg){fig-alt=\"Alt text\"}](https://jaeho-jung.github.io/)"
  },
  {
    "objectID": "posts/Markdown/markdown_basics.html#references",
    "href": "posts/Markdown/markdown_basics.html#references",
    "title": "Markdown 기초 문법",
    "section": "References",
    "text": "References\nhttps://www.markdownguide.org/getting-started/\nhttps://www.markdownguide.org/basic-syntax/\nhttps://www.markdownguide.org/extended-syntax/#fenced-code-blocks\nhttps://quarto.org/docs/authoring/markdown-basics.html\nhttps://ko.wikipedia.org/wiki/%EB%A7%88%ED%81%AC%EB%8B%A4%EC%9A%B4"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/Crawler_Issue.html",
    "href": "posts/Information_Retrieval/Crawler/Crawler_Issue.html",
    "title": "Crawler의 Issue",
    "section": "",
    "text": "Where to Start\nLink ordering\n\n\nDFS (LIFO)\nBFS (FIFO)\nBest-first Algorithm\nFish\n\n\nCircularities\nDuplicates\nChecking for changes\n\n\n\n\n\n\nBe immune to spider traps and other malcious behavior from web servers\n\n\n\n\n\n\n\n\n\n\nhttp://www.robotstxt.org/\nThe Robots Exclusion Protocol.\n\n\n정의: 사이트나 문서에 대한 로봇들의 접근을 제어하기 위한 표준 규약이다.\n\n웹사이트의 어떤 부분을 크롤링해도 되는지에 대한 명세로, 웹사이트 관리자가 작성한다.\n\n작동 방식: 로봇이 웹사이트의 URL에 접근하기 전에, 해당 웹사이트의 robots.txt 파일을 확인하여 규약에 따른다.\n위치: 웹 서버 최상위 경로에 위치한다.\n\ne.g. https://www.google.com/robots.txt\n\n\nExample\nUser-agent: *\nDisallow: /yoursite/temp/\n\nUser-agent: searchengine\nDisallow: \n\n\n\n\n\ndon’t hit a server too often\nRespect implicit and explicit politeness considerations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nincl dynamically generated\ninfinite URL names\nIll-formed HTML\n\nE.g. page with 68kB of null characters\n\nMisleading sites\n\nindefinite number of pages dynamically generated by CGI scripts\npaths of arbitrary depth created using soft directory links and path remapping features in HTTP server\n\n\nSolution\n\nCheck for URL length\nGuards\n\nregular crawl statistics\nAdding dominating sites to guard module\nCGI form quaries와 같은 내용의 크롤링 비활성화\n텍스트 데이터 타입이 아닌 URL 제거\n\n\n\n\n\n\n\nContinue fetching fresh copies of a previously fetched page\n\n\n\n\nAdapt to new data formats, protocols\n\n\n\n\n\n일반적인 HTML 웹 페이지는 압축 시 2-4 kB\n\nzlib\n\nDeflate 압축 알고리즘을 C언어로 구현한 라이브러리\n비손실 압축 알고리즘\n높은 이식성\n압축 알고리즘계의 산업 표준\n\n\n작은 규모의 시스템\n\nStorage Manager 사용 (E.g. Berkeley DB)\n디스크 기반의 데이터베이스 관리\nconfiguration as a hash-table/B-tree for URL access key\nconfiguration as a sequential log of page records.\n\n큰 규모의 시스템"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/Crawler_Issue.html#issue",
    "href": "posts/Information_Retrieval/Crawler/Crawler_Issue.html#issue",
    "title": "Crawler의 Issue",
    "section": "",
    "text": "Where to Start\nLink ordering\n\n\nDFS (LIFO)\nBFS (FIFO)\nBest-first Algorithm\nFish\n\n\nCircularities\nDuplicates\nChecking for changes\n\n\n\n\n\n\nBe immune to spider traps and other malcious behavior from web servers\n\n\n\n\n\n\n\n\n\n\nhttp://www.robotstxt.org/\nThe Robots Exclusion Protocol.\n\n\n정의: 사이트나 문서에 대한 로봇들의 접근을 제어하기 위한 표준 규약이다.\n\n웹사이트의 어떤 부분을 크롤링해도 되는지에 대한 명세로, 웹사이트 관리자가 작성한다.\n\n작동 방식: 로봇이 웹사이트의 URL에 접근하기 전에, 해당 웹사이트의 robots.txt 파일을 확인하여 규약에 따른다.\n위치: 웹 서버 최상위 경로에 위치한다.\n\ne.g. https://www.google.com/robots.txt\n\n\nExample\nUser-agent: *\nDisallow: /yoursite/temp/\n\nUser-agent: searchengine\nDisallow: \n\n\n\n\n\ndon’t hit a server too often\nRespect implicit and explicit politeness considerations\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nincl dynamically generated\ninfinite URL names\nIll-formed HTML\n\nE.g. page with 68kB of null characters\n\nMisleading sites\n\nindefinite number of pages dynamically generated by CGI scripts\npaths of arbitrary depth created using soft directory links and path remapping features in HTTP server\n\n\nSolution\n\nCheck for URL length\nGuards\n\nregular crawl statistics\nAdding dominating sites to guard module\nCGI form quaries와 같은 내용의 크롤링 비활성화\n텍스트 데이터 타입이 아닌 URL 제거\n\n\n\n\n\n\n\nContinue fetching fresh copies of a previously fetched page\n\n\n\n\nAdapt to new data formats, protocols\n\n\n\n\n\n일반적인 HTML 웹 페이지는 압축 시 2-4 kB\n\nzlib\n\nDeflate 압축 알고리즘을 C언어로 구현한 라이브러리\n비손실 압축 알고리즘\n높은 이식성\n압축 알고리즘계의 산업 표준\n\n\n작은 규모의 시스템\n\nStorage Manager 사용 (E.g. Berkeley DB)\n디스크 기반의 데이터베이스 관리\nconfiguration as a hash-table/B-tree for URL access key\nconfiguration as a sequential log of page records.\n\n큰 규모의 시스템"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/Crawler_Issue.html#implementation",
    "href": "posts/Information_Retrieval/Crawler/Crawler_Issue.html#implementation",
    "title": "Crawler의 Issue",
    "section": "4. Implementation",
    "text": "4. Implementation\n\nURL Frontier에 seed URL이 들어있다.\nFrontier에서 URL을 하나 추출한다. 2.1. URL에서 문서를 Fetch한다. 2.1. Parse the URL - 문서에서 URL을 추출한다. 2.2. 이미 방문한 URL인지 확인한다.\nFor each extracted URL 3.1. URL 필터 테스트 (e.g. .edu 만 크롤링한다, robots.txt를 준수하는지) 3.2. 이미 URL Frontier에 존재하는지 확인"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/Crawler_Issue.html#strategy",
    "href": "posts/Information_Retrieval/Crawler/Crawler_Issue.html#strategy",
    "title": "Crawler의 Issue",
    "section": "5. Strategy",
    "text": "5. Strategy\n\nURL Frontier\n\n\n같은 호스트의 여러 페이지들을 포함할 수 있다.\n모든 페이지들을 한 번에 Fetch 하는 것을 반드시 피해야 한다.\n모든 크롤링 thread가 바빠야 한다.\n\n\n\nDNS (Domain Name Server)\n\n\n인터넷의 lookup 서비스\n\nURL이 주어지면, IP를 반환한다.\n서비스가 분산 서버에 의해 제공되므로 lookup latency가 높을 수 있다.\n\nDNS lookup 은 일반적으로 blocking으로 구현된다. - ??\n\nonly one outstanding request at a time\n\n해결법\n\nDNS caching\nBatch DNS resolver\n\nrequest들을 모아서 같이 전송\n\n\n\n\n\nfetching\n\n\n한 번에 많은 페이지를 Fetch해야 함\n가능한 많이 cache해야 함\n\nDNS\nrobots.txt\nDocument (for later processing)\n\nDefensive\n\nTime out http connections\n\"crawler traps\"\nURL filter\nCheckpointing\n\n\n\n\nParsing\n\n\nAnchor tag: &lt;a href=\"URL\"...&gt;\nOption tag: &lt;option value=\"URL\"...&gt;\nMap: &lt;area href=\"URL\"...&gt;\nFrame: &lt;frame src=\"URL\"...&gt;\nLink to an image: &lt;img src=\"URL\"...&gt;\nAbsolute path: &lt;base href=...&gt;\n\n\nURL normalization\n\nrelative URL 을 normalize(expand) 해준다.\n\n\n\n\nFilter\n\n\nRegular Expression\nCache robots.txt files\n\n\n\nDistribution – HOW?\n\n\n서로 다른 작업들에 대해 다중 크롤러 스레드를 실행\n\n\n\nDuplicates\n\n\nURL-seen test\n\n\nContent-seen test\ndifferent URL, same document\n\n웹 상에는 중복 문서가 다수 존재한다.\n\nDuplication\n\nExact match\n많지 않음\n\nNear-Duplication\n\nApproximate match\n다수 존재\nE.g. Last modified date만 다른 경우\nsyntatic similarity\nedit-distance measure\nsimilarity threshold\nE.g. if Similarity &gt; 80% =&gt; near duplicates"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/Crawler_Issue.html#open-source-cralwers",
    "href": "posts/Information_Retrieval/Crawler/Crawler_Issue.html#open-source-cralwers",
    "title": "Crawler의 Issue",
    "section": "Open Source Cralwers",
    "text": "Open Source Cralwers\n\nheritix\n\n\n\napache nutch\n\n\n\nScrapy"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/Crawler_Issue.html#focused-crawler",
    "href": "posts/Information_Retrieval/Crawler/Crawler_Issue.html#focused-crawler",
    "title": "Crawler의 Issue",
    "section": "Focused Crawler",
    "text": "Focused Crawler\n\nIssue\n\n\n관련 정보가 포함될 가능성이 가장 높은 URL 식별\n관련이 없거나 품질이 나쁜 문서를 피해야 함\n\n\n\nTechniques\n\n\n1. Lexical Based Approach\n\nFish Search System\nCrawling algorithm based on depth-first search. Heuristics determine the selection of the documents that are to be retrieved.\n\n\nShark Search System\nBased on a fuzzy score to replace the binary relevant/irrelevant) evaluation of document relevance, i.e., a score between 0 and 1 (0 for no similarity whatsoever, 1 for perfect “conceptual” match) rather than a binary value\n\n\nFocused Crawler based on Category Taxonomy\nBased on a category tree based document taxonomy and seed documents which build a model for classification of retrieved pages into categories. Uses semi-supervised learning\n\n\nFocused Crawler based on Similarity Computing Engine\nRetrieves information related to a user-specified topic from the Web using TF.IDF weighting scheme to find the topic keywords set to represent the topic, and uses vector space model to determine whether web pages are relevant to the topic or not.\n\n\n\n2. Link Based Approach\n\nSimilarity based Crawler\nCalculates the Page rank score on the graph induced by pages downloaded so far and then using this score as a priority of URLs extracted from a page.\n\n\nHITS\nIdentifies the web page context based on Authority Score and Hub score associated with the web page. The basic principle here is the following mutually reinforcing relationship between hubs and authorities. A good hub page points to many good authority pages. A good authority page is pointed to by many good hub pages.\n\n\nARC\nAutomatically compiles a resource list on any topic that is broad and well-represented on the web. Based on modified weighted Authority Score and Hub score. Uses Anchor window wherenin the system looks on either side of the href for a window of B bytes, to increase the authority weight.\n\n\nDOM based Focused Crawler\nLocates regions and subtrees of pages using DOM tree of a web page, gets favorable treatment in propagating link-based popularity and implicitly suppressing propagation of popularity to regions with noisy links. Identifies and extract hub regions relevant to a topic and guides the hub and authority reinforcement to work on a selected, highly relevant subgraph of the web.\n\n\nContext Graph based Focused Crawler\nUses a context graph to train a set of classifiers to assign documents to different categories based on their expected link distance to the target. Naïve Bayes classifier is used for each layer of the graph."
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/Crawler_Issue.html#references",
    "href": "posts/Information_Retrieval/Crawler/Crawler_Issue.html#references",
    "title": "Crawler의 Issue",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/Crawler_Policy.html",
    "href": "posts/Information_Retrieval/Crawler/Crawler_Policy.html",
    "title": "Crawling Policy",
    "section": "",
    "text": "Selection policy: 다운로드할 페이지를 지정하는 정책\nRe-visit policy: 페이지 변경 확인 시기를 지정하는 정책\nPoliteness policy: 웹사이트 과부하를 피하는 방법을 지정하는 정책\nParallelization policy: 분산 웹 크롤러를 조율하는 방법하는 지정하는 정책"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/Crawler_Policy.html#크롤링-정책",
    "href": "posts/Information_Retrieval/Crawler/Crawler_Policy.html#크롤링-정책",
    "title": "Crawling Policy",
    "section": "",
    "text": "Selection policy: 다운로드할 페이지를 지정하는 정책\nRe-visit policy: 페이지 변경 확인 시기를 지정하는 정책\nPoliteness policy: 웹사이트 과부하를 피하는 방법을 지정하는 정책\nParallelization policy: 분산 웹 크롤러를 조율하는 방법하는 지정하는 정책"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/Crawler_Policy.html#selection-policy",
    "href": "posts/Information_Retrieval/Crawler/Crawler_Policy.html#selection-policy",
    "title": "Crawling Policy",
    "section": "Selection policy",
    "text": "Selection policy\n웹의 현재 규모를 고려할 때 대형 검색 엔진조차도 공개적으로 이용 가능한 부분만을 다루고 있다. 2009년의 연구에 따르면 대형 검색 엔진도 색인 가능한 웹의 40–70%를 넘지 않았다. 크롤러는 항상 웹 페이지의 일부만을 다운로드하므로, 가장 관련성 높은 페이지를 다운로드하는 것이 중요하다. 페이지의 중요성은 본질적인 품질, 링크나 방문 횟수에 대한 인기, 심지어는 URL에 따라 달라진다.\n\n링크 제한\n\n크롤러는 HTML 페이지만 찾고 다른 MIME 유형은 피하고 싶을 수 있다.\nHTML 자원만 요청하려면 HTTP HEAD 요청을 사용하여 웹 자원의 MIME 유형을 확인한 후 GET 요청으로 전체 자원을 요청할 수 있다.\n많은 HEAD 요청을 피하기 위해 URL을 검사하여 URL이 특정 문자로 끝나는 경우에만 자원을 요청할 수 있다.\n이 전략은 의도하지 않게 많은 HTML 웹 자원을 건너뛸 수 있다.\n\n\n\nURL 정규화\n\n크롤러는 동일한 자원을 여러 번 크롤링을 피하기 위해 URL 정규화를 수행한다.\nURL 정규화는 URL을 일관된 방식으로 수정하고 표준화하는 프로세스를 의미한다.\n소문자로 변환, “.” 및 “..” 세그먼트 제거, 비어 있지 않은 경로 구성 요소에 대한 슬래시 추가 등 여러 유형의 정규화를 수행할 수 있다.\n\n\n\n경로 상승 크롤링\n\n일부 크롤러는 특정 웹 사이트에서 가능한 많은 자원을 다운로드하려고 한다.\n경로 상승 크롤러는 크롤하려는 각 URL의 모든 경로로 상승하는 크롤러로 효과적으로 사용된다.\n예를 들어, 주어진 시드 URL이 http://llama.org/hamster/monkey/page.html이면 /hamster/monkey/, /hamster/, / 등을 크롤하려고 시도할 것이다.\n경로 상승 크롤러는 정규 크롤링에서 발견할 수 없는 고립된 자원이나 연결이 없는 자원을 효과적으로 찾는 데 사용된다.\n\n\n\n포커스 크롤링\n\n크롤러에 대한 페이지의 중요성은 페이지의 텍스트와 주어진 쿼리 간의 유사성 함수로 나타낼 수 있다.\n유사한 페이지를 다운로드하려는 웹 크롤러는 포커스 크롤러 또는 주제 크롤러라고 한다.\n포커스 크롤링은 페이지의 텍스트와 쿼리 간의 유사성을 실제로 페이지를 다운로드하기 전에 예측할 수 있는 능력이 주요한 문제이다.\n일부 예측자로는 링크의 앵커 텍스트가 있으며, 다른 방법으로는 이미 방문한 페이지의 완전한 콘텐츠를 사용하여 운전 쿼리와 아직 방문하지 않은 페이지 간의 유사성을 추론한다.\n\n\n\n학술 포커스 크롤러\n\n학술 크롤러는 학문적인 관련 문서를 크롤링하는 예이다.\nCiteseerxbot과 같은 학문 검색 엔진의 크롤러는 무료로 이용 가능한 학술 문서를 크롤링한다.\n학술 크롤러는 PDF, PostScript 파일, Microsoft Word 등과 같은 다양한 형식의 문서를 크롤링하는 데 관심이 있다.\n일반적인 오픈 소스 크롤러는 다른 MIME 유형을 필터링하기 위해 사용자 정의되어야 하거나 미들웨어를 사용하여 문서를 추출하고 포커스 크롤 데이터베이스로 가져와야 할 수 있다.\n\n\n\n의미론적 포커스 크롤러\n\n의미론적 포커스 크롤러는 도메인 온톨로지를 사용하여 주제 지도를 나타내고 웹 페이지를 선택 및 분류하는 데 사용한다.\n온톨로지는 크롤링 프로세스에서 자동으로 업데이트될 수 있다.\n의미론적 온톨로지를 기반으로 하는 크롤러는 지식 벡터 머신을 사용하여 웹 페이지를 크롤하는 동안 온톨로지 개념의 내용을 업데이트하는 방법을 소개한다."
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/Crawler_Policy.html#references",
    "href": "posts/Information_Retrieval/Crawler/Crawler_Policy.html#references",
    "title": "Crawling Policy",
    "section": "References",
    "text": "References\nhttps://en.wikipedia.org/wiki/Web_crawler"
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/Crawler_Policy.html#re-visit-policy",
    "href": "posts/Information_Retrieval/Crawler/Crawler_Policy.html#re-visit-policy",
    "title": "Crawling Policy",
    "section": "Re-visit policy",
    "text": "Re-visit policy\n웹은 매우 동적인 성격을 가지고 있다. 웹 크롤러가 크롤을 완료할 때 까지 create, update, delete 등의 이벤트가 발생할 수 있다.\n검색 엔진 관점에서 이러한 이벤트를 감지하지 못하고, 오래된 리소스를 저장한다면 비용이 발생한다.\n비용을 계산하는 함수로 흔히 freshness, age가 사용된다.\n\nFreshness\n- 로컬 복사본이 정확한지 여부를 나타내는 2진 측정치\n- 시간 t에 저장소에 있는 p의 freshness는 다음과 같이 정의된다.\n\\[\nF_{p}(t) = \\begin{cases} 1 & \\text{if } p \\text{ is equal to the local copy at time } t \\\\ 0 & \\text{otherwise} \\end{cases} \\\n\\]\n\n\nAge\n- 로컬 복사본이 얼마나 오래되었는지를 나타내는 측정치\n- 시간 t에 저장소에 있는 페이지 p의 age는 다음과 같이 정의된다.\n\\[\nA_{p}(t) = \\begin{cases} 0 & \\text{if } p \\text{ is not modified at time } t \\\\ t - \\text{modification time of } p & \\text{otherwise} \\end{cases} \\\n\\]\nCoffman 등은 웹 크롤러가 서버이고 웹 사이트가 대기열인 다중 대기열, 단일 서버 폴링 시스템으로 모델링할 수 있다고 제안했다. 페이지 수정은 고객의 도착이고, 스위치 오버 시간은 단일 웹 사이트에 대한 페이지 액세스 간격이다. 이 모델에서 폴링 시스템의 평균 대기 시간은 웹 크롤러의 평균 age와 동일하다.\n웹 크롤러의 목적은 컬렉션 내 페이지의 평균 freshness를 가능한 한 높이거나 페이지의 평균 age을 가능한 한 낮추는 것이다.\n웹 크롤러에서 신선도와 연령의 진화\nCho와 Garcia-Molina는 두 가지 간단한 re-visit policy를 연구했다:\n\n균일 정책: 변화율과 상관없이 컬렉션의 모든 페이지를 동일한 빈도로 재방문한다.\n비례 정책: 변화율이 높은 페이지를 더 자주 재방문한다. 방문 빈도는 추정된 변화 빈도에 비례한다.\n\n양쪽 모두에서 페이지의 반복 크롤링 순서는 무작위 또는 고정 순서로 수행될 수 있다.\n결과\nCho와 Garcia-Molina는 균일 정책이 시뮬레이션 및 실제 웹 크롤에서 신선도의 평균 측면에서 비례 정책을 능가한다는 것을 입증했다.\n웹 크롤러는 주어진 시간 프레임 내에 크롤링 할 수 있는 페이지의 한계가 있기 때문에\n1. 자주 업데이트되지 않는 페이지의 비용을 희생시켜 자주 업데이트되는 페이지에 지나치게 많은 새로운 크롤을 할당한다.\n2. 자주 변경되는 페이지의 신선도는 자주 변경되지 않는 페이지보다 짧다.\n즉, 비례 정책은 자주 업데이트되는 페이지에 더 많은 리소스를 할당하지만 전반적인 freshness time은 적다.\n신선도를 향상시키기 위해, 크롤러는 너무 자주 변경되는 요소에 벌을 주어야 한다.\n\n평균 freshness를 높이기 위한 최적의 방법은 너무 자주 변경되는 페이지를 무시하는 것이다.\n평균 age를 낮추기 위한 최적의 방법은 각 페이지의 변경 속도와 단조적으로(하위 선형으로) 증가하는 액세스 빈도를 사용하는 것이다.\n여기서 고려된 재방문 정책은 모든 페이지를 품질 측면에서 동질적으로 간주한다.- 웹 페이지 품질에 대한 추가 정보를 포함해야 더 나은 크롤링 정책을 달성할 수 있다."
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/Crawler_Policy.html#politeness-policy",
    "href": "posts/Information_Retrieval/Crawler/Crawler_Policy.html#politeness-policy",
    "title": "Crawling Policy",
    "section": "Politeness policy",
    "text": "Politeness policy\n웹 크롤러는 사람보다 훨씬 빠르고 깊은 수준의 데이터를 검색할 수 있기 때문에 사이트의 성능에 치명적인 영향을 미칠 수 있다. 단일 크롤러가 초당 여러 요청을 수행하거나 대용량 파일을 다운로드하는 경우, 서버는 여러 크롤러의 요청에 대응하기 어려울 수 있다.\n\nKoster에 따르면 웹 크롤러의 사용은 여러 작업에 유용하지만 일반 커뮤니티에는 비용이 따른다. 웹 크롤러 사용의 비용에는 다음이 포함됩니다:\n\n\n네트워크 자원: 크롤러는 상당한 대역폭이 필요하며 긴 시간 동안 고도로 병렬로 작동합니다.\n서버 과부하: 특히 특정 서버로의 액세스 빈도가 너무 높은 경우.\n잘못 작성된 크롤러: 서버나 라우터를 충돌시키거나 처리할 수 없는 페이지를 다운로드하는 크롤러.\n개인 크롤러: 너무 많은 사용자가 배치되면 네트워크와 웹 서버를 방해할 수 있다.\n\n\n\n\n\n\n\nrobots.txt\n\n\n\n이러한 문제에 대한 부분적인 해결책은 robots.txt 프로토콜로, 관리자가 크롤러에 의해 액세스되지 말아야 할 웹 서버의 부분을 지정하는 표준이다.\n\n최근 Google, Ask Jeeves, MSN 및 Yahoo! Search와 같은 상업용 검색 엔진은 robots.txt 파일에 Crawl-delay: 매개변수를 추가하여 요청 간의 지연 시간을 나타낼 수 있게 되었다.\n\n\n\n액세스 간격 - Cho: 10초 - WIRE crawler: 15초 - MercatorWeb crawler: 적응적인 politeness policy - 특정 서버에서 문서를 다운로드하는 데 t초가 걸린 경우 다음 페이지를 다운로드하기 전에 10t초 기다린다. - Dill:1초\n연구 목적으로 웹 크롤러를 사용하는 경우 더 자세한 비용 대 혜택 분석이 필요하며 어디를 크롤할지, 얼마나 빨리 크롤할지 결정할 때 윤리적 측면을 고려해야 한다."
  },
  {
    "objectID": "posts/Information_Retrieval/Crawler/Crawler_Policy.html#parallelization-policy",
    "href": "posts/Information_Retrieval/Crawler/Crawler_Policy.html#parallelization-policy",
    "title": "Crawling Policy",
    "section": "Parallelization policy",
    "text": "Parallelization policy\n\n병렬 크롤링: 여러 프로세스를 사용하여 동시에 크롤링을 수행하는 방식.\n목표: 다운로드 속도를 극대화하고 병렬화로 인한 오버헤드를 최소화하며 동일한 페이지의 반복 다운로드를 방지.\n필요성: 동일한 URL이 두 개의 다른 크롤링 프로세스에서 발견될 수 있으므로, 새로 발견된 URL을 할당하는 정책이 필요함."
  }
]